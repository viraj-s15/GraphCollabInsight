{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Creation\n",
    "This notebook, consist of creating the architecture of the neural network and training it, once, we have finished that, we will save the model\n",
    "as a .pt file and then use it for inference  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "\n",
    "import dgl\n",
    "import dgl.data\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Documentation: Initializing Reproducibility Seeds\n",
    "\n",
    "This code snippet defines a function `setup_seed` to establish seeds for random number generators, \n",
    "ensuring reproducibility in experiments involving randomness.\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "def setup_seed(self, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(self, seed):\n",
    "  \"\"\"\n",
    "  Setting seeds for reproducible results\n",
    "  \"\"\"\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "  dgl.seed(seed)\n",
    "  dgl.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing\n",
    "\n",
    "Loading in the custom dataset which we created into dgl, since we have already formed the data into the correct format,\n",
    "we can use the built in graph class without any modifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n",
      "Graph(num_nodes=333, num_edges=2852,\n",
      "      ndata_schemes={'feat': Scheme(shape=(224,), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CSVDataset('../data/author_data')\n",
    "g = dataset[0]\n",
    "g = dgl.add_self_loop(g)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block snippet splits the edge set of a graph into training and testing sets for link prediction tasks.\n",
    "\n",
    "1. **Edge Set Preparation:**\n",
    "   - Extract source (`u`) and destination (`v`) nodes from the graph's edges.\n",
    "   - Generate a random permutation of edge indices (`eids`) to shuffle the edge set.\n",
    "   - Determine the sizes of the test and training sets.\n",
    "\n",
    "2. **Positive Edge Splitting:**\n",
    "   - Extract source and destination nodes for the test positive edges.\n",
    "   - Extract source and destination nodes for the train positive edges.\n",
    "\n",
    "3. **Negative Edge Generation:**\n",
    "   - Create an adjacency matrix (`adj`) from the edge information.\n",
    "   - Calculate the adjacency matrix for negative edges (`adj_neg`) by subtracting the positive adjacency matrix from a matrix of ones.\n",
    "   - Identify source and destination nodes for negative edges.\n",
    "\n",
    "4. **Negative Edge Splitting:**\n",
    "   - Randomly sample negative edges for the test set.\n",
    "   - Assign source and destination nodes for the test negative edges.\n",
    "   - Assign source and destination nodes for the train negative edges.\n",
    "\n",
    "This code prepares positive and negative edge sets for training and testing link prediction tasks. It facilitates the creation of a comprehensive dataset for evaluating graph models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.num_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * 0.1)\n",
    "train_size = g.num_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.num_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), g.num_edges())\n",
    "test_neg_u, test_neg_v = (\n",
    "    neg_u[neg_eids[:test_size]],\n",
    "    neg_v[neg_eids[:test_size]],\n",
    ")\n",
    "train_neg_u, train_neg_v = (\n",
    "    neg_u[neg_eids[test_size:]],\n",
    "    neg_v[neg_eids[test_size:]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, you will need to remove the edges in the test set from the original graph. You can do this via `dgl.remove_edges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = dgl.remove_edges(g, eids[:test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our graph sage network\n",
    "\n",
    "1. **Import GraphSAGE Convolution:**\n",
    "   - Import `SAGEConv` from the DGL's neural network module.\n",
    "\n",
    "2. **Model Definition:**\n",
    "   - Define a two-layer GraphSAGE model class (`GraphSAGE`) that inherits from `nn.Module`.\n",
    "   - Constructor (`__init__`):\n",
    "       - Initialize the model using input feature dimensions (`in_feats`) and hidden feature dimensions (`h_feats`).\n",
    "       - Create two `SAGEConv` layers: the first with input features, the second with hidden features.\n",
    "   - Forward Pass (`forward`):\n",
    "       - Execute the forward computation of the model.\n",
    "       - Perform GraphSAGE convolution on the input graph `g` and input features `in_feat`.\n",
    "       - Apply ReLU activation function to intermediate results.\n",
    "       - Perform the second GraphSAGE convolution on the updated features and return the result.\n",
    "\n",
    "This code defines a GraphSAGE model architecture for graph-based learning tasks, facilitating the propagation of node features through the graph structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, \"mean\")\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more comlp[ex version of  the above class and we could have used it, however, the data thatw e have been\n",
    "provided with is extremely clean and does not need a very complex model architecture for training. \n",
    "```python\n",
    "class ComplexGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_layers, num_hidden, num_classes, dropout):\n",
    "        super(ComplexGraphSAGE, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.linears = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.convs.append(SAGEConv(in_feats, h_feats, 'mean'))\n",
    "        self.linears.append(nn.Linear(in_feats, h_feats))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for layer in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(h_feats, h_feats, 'mean'))\n",
    "            self.linears.append(nn.Linear(h_feats, h_feats))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_linear = nn.Linear(h_feats, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = in_feat\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            conv = self.convs[layer]\n",
    "            linear = self.linears[layer]\n",
    "            \n",
    "            h_conv = conv(g, h)\n",
    "            h_linear = linear(h)\n",
    "            \n",
    "            h = F.relu(h_conv + h_linear)  # Skip connection\n",
    "            \n",
    "            if layer < self.num_layers - 1:\n",
    "                h = self.dropout(h)\n",
    "        \n",
    "        h = self.output_linear(h)\n",
    "        return h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet creates graph structures for training and testing positive and negative edges.\n",
    "\n",
    "1. **Training Positive Graph:**\n",
    "   - Create a DGL graph `train_pos_g` using positive training edges `train_pos_u` and `train_pos_v`.\n",
    "   - Specify the number of nodes as `g.num_nodes()`.\n",
    "\n",
    "2. **Training Negative Graph:**\n",
    "   - Create a DGL graph `train_neg_g` using negative training edges `train_neg_u` and `train_neg_v`.\n",
    "   - Specify the number of nodes as `g.num_nodes()`.\n",
    "\n",
    "3. **Testing Positive Graph:**\n",
    "   - Create a DGL graph `test_pos_g` using positive testing edges `test_pos_u` and `test_pos_v`.\n",
    "   - Specify the number of nodes as `g.num_nodes()`.\n",
    "\n",
    "4. **Testing Negative Graph:**\n",
    "   - Create a DGL graph `test_neg_g` using negative testing edges `test_neg_u` and `test_neg_v`.\n",
    "   - Specify the number of nodes as `g.num_nodes()`.\n",
    "\n",
    "This code generates separate graph structures for training and testing positive and negative edges, facilitating the evaluation of the GraphSAGE model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.num_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.num_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.num_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.num_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ca use a generic dot product method that is given by the class is well and that would be recommended as it has been\n",
    "optimised for large workflows. Our dataset is rather small and I wanted to implement a custom predictor  \n",
    "\n",
    "1. **MLP Predictor Class:**\n",
    "   - Define a class `MLPPredictor` that inherits from `nn.Module`.\n",
    "   - Constructor (`__init__`):\n",
    "       - Initialize the model with hidden feature dimensions (`h_feats`).\n",
    "       - Create two linear layers: `W1` and `W2`.\n",
    "   - `apply_edges` Method:\n",
    "       - Compute scalar scores for each edge in the graph.\n",
    "       - Concatenate source and destination node features and pass through `W1` and `W2`.\n",
    "   - `forward` Method:\n",
    "       - Perform the forward computation.\n",
    "       - Set node features in the graph (`g.ndata[\"h\"]`) as input node features (`h`).\n",
    "       - Apply the `apply_edges` method to calculate edge scores.\n",
    "       - Return edge scores.\n",
    "\n",
    "2. **Important Information:**\n",
    "   - The `apply_edges` method computes a scalar score for each edge using node features.\n",
    "   - The `forward` method applies the predictor to the graph using node features.\n",
    "   - This code segment showcases how to create a custom predictor for link prediction tasks in GNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        \"\"\"\n",
    "        Computes a scalar score for each edge of the given graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges :\n",
    "            Has three members ``src``, ``dst`` and ``data``, each of\n",
    "            which is a dictionary representing the features of the\n",
    "            source nodes, the destination nodes, and the edges\n",
    "            themselves.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary of new edge features.\n",
    "        \"\"\"\n",
    "        h = torch.cat([edges.src[\"h\"], edges.dst[\"h\"]], 1)\n",
    "        return {\"score\": self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata[\"h\"] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "\n",
    "- **Model Initialization (`model`):**\n",
    "   - Initialize a `GraphSAGE` model (`GraphSAGE`) with input feature dimensions and hidden feature dimensions.\n",
    "   - The input feature dimensions are determined by `train_g.ndata[\"feat\"].shape[1]`.\n",
    "   - Hidden feature dimensions are set to 16.\n",
    "\n",
    "- **Predictor Initialization (`pred`):**\n",
    "   - Initialize an `MLPPredictor` model (`MLPPredictor`) with 16 hidden feature dimensions.\n",
    "   - The predictor computes scalar scores for link prediction.\n",
    "\n",
    "### Loss and AUC Functions\n",
    "\n",
    "- **Loss Computation (`compute_loss`):**\n",
    "   - Compute the binary cross-entropy loss between positive and negative scores.\n",
    "   - Concatenate positive and negative scores and corresponding labels.\n",
    "   - Utilize `F.binary_cross_entropy_with_logits` to compute the loss.\n",
    "\n",
    "- **AUC Computation (`compute_auc`):**\n",
    "   - Compute the Area Under the Curve (AUC) for evaluating model performance.\n",
    "   - Concatenate positive and negative scores and corresponding labels.\n",
    "   - Calculate AUC using the `roc_auc_score` function from `sklearn.metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(train_g.ndata[\"feat\"].shape[1], 16)\n",
    "# You can replace DotPredictor with MLPPredictor.\n",
    "pred = MLPPredictor(16)\n",
    "\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    )\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    ).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the type of the `feat` from Long to float32 as that siw aht is expected by our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = model(train_g, train_g.ndata[\"feat\"])\n",
    "train_g.ndata['feat'] = train_g.ndata['feat'].type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Optimizer Initialization (`optimizer`):**\n",
    "   - Initialize an Adam optimizer for both the GNN model and the predictor.\n",
    "   - Combines the parameters from both using `itertools.chain`.\n",
    "   - Learning rate is set to 0.01.\n",
    "\n",
    "2. **Training Loop (`for e in range(350):`):**\n",
    "   - Perform 350 training epochs.\n",
    "   - Forward Pass:\n",
    "       - Execute the GNN model (`model`) on the training graph using input node features.\n",
    "       - Calculate scores for positive and negative edges using the predictor (`pred`).\n",
    "       - Compute the loss using the `compute_loss` function.\n",
    "   - Backward Pass:\n",
    "       - Reset optimizer gradients.\n",
    "       - Perform backpropagation to compute gradients.\n",
    "       - Update model parameters using the optimizer.\n",
    "   - Print loss every 5 epochs.\n",
    "\n",
    "3. **AUC Evaluation (`with torch.no_grad():`):**\n",
    "   - Compute AUC for model evaluation.\n",
    "   - Calculate scores for positive and negative edges using the predictor.\n",
    "   - Utilize `compute_auc` function to compute the AUC.\n",
    "\n",
    "4. **Note:**\n",
    "   - The training loop optimizes the model using the computed loss.\n",
    "   - After training, the code evaluates the trained model's performance using AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.8466184139251709\n",
      "In epoch 5, loss: 0.788202702999115\n",
      "In epoch 10, loss: 0.8662614226341248\n",
      "In epoch 15, loss: 0.7980585098266602\n",
      "In epoch 20, loss: 0.7788756489753723\n",
      "In epoch 25, loss: 0.6864426732063293\n",
      "In epoch 30, loss: 0.5877717137336731\n",
      "In epoch 35, loss: 0.49723267555236816\n",
      "In epoch 40, loss: 0.41885870695114136\n",
      "In epoch 45, loss: 0.38718992471694946\n",
      "In epoch 50, loss: 0.3665114641189575\n",
      "In epoch 55, loss: 0.34574928879737854\n",
      "In epoch 60, loss: 0.3321414887905121\n",
      "In epoch 65, loss: 0.32042211294174194\n",
      "In epoch 70, loss: 0.31038662791252136\n",
      "In epoch 75, loss: 0.3018914759159088\n",
      "In epoch 80, loss: 0.29395124316215515\n",
      "In epoch 85, loss: 0.28652819991111755\n",
      "In epoch 90, loss: 0.2790970206260681\n",
      "In epoch 95, loss: 0.27185624837875366\n",
      "In epoch 100, loss: 0.26480740308761597\n",
      "In epoch 105, loss: 0.2581842839717865\n",
      "In epoch 110, loss: 0.2520672082901001\n",
      "In epoch 115, loss: 0.2462194859981537\n",
      "In epoch 120, loss: 0.24323329329490662\n",
      "In epoch 125, loss: 0.23587282001972198\n",
      "In epoch 130, loss: 0.2334723174571991\n",
      "In epoch 135, loss: 0.22711043059825897\n",
      "In epoch 140, loss: 0.2222817838191986\n",
      "In epoch 145, loss: 0.21768394112586975\n",
      "In epoch 150, loss: 0.21396459639072418\n",
      "In epoch 155, loss: 0.21015973389148712\n",
      "In epoch 160, loss: 0.20536074042320251\n",
      "In epoch 165, loss: 0.20178242027759552\n",
      "In epoch 170, loss: 0.19822287559509277\n",
      "In epoch 175, loss: 0.19466914236545563\n",
      "In epoch 180, loss: 0.19191238284111023\n",
      "In epoch 185, loss: 0.19093482196331024\n",
      "In epoch 190, loss: 0.18562453985214233\n",
      "In epoch 195, loss: 0.18378964066505432\n",
      "In epoch 200, loss: 0.18131019175052643\n",
      "In epoch 205, loss: 0.1769053041934967\n",
      "In epoch 210, loss: 0.17756931483745575\n",
      "In epoch 215, loss: 0.17332899570465088\n",
      "In epoch 220, loss: 0.1703605055809021\n",
      "In epoch 225, loss: 0.17059794068336487\n",
      "In epoch 230, loss: 0.16581371426582336\n",
      "In epoch 235, loss: 0.16356854140758514\n",
      "In epoch 240, loss: 0.1625577062368393\n",
      "In epoch 245, loss: 0.1622268408536911\n",
      "In epoch 250, loss: 0.1576514095067978\n",
      "In epoch 255, loss: 0.15653906762599945\n",
      "In epoch 260, loss: 0.15602412819862366\n",
      "In epoch 265, loss: 0.15176618099212646\n",
      "In epoch 270, loss: 0.1514003723859787\n",
      "In epoch 275, loss: 0.14972099661827087\n",
      "In epoch 280, loss: 0.14654980599880219\n",
      "In epoch 285, loss: 0.14750228822231293\n",
      "In epoch 290, loss: 0.14352555572986603\n",
      "In epoch 295, loss: 0.14301709830760956\n",
      "In epoch 300, loss: 0.14253197610378265\n",
      "In epoch 305, loss: 0.13871073722839355\n",
      "In epoch 310, loss: 0.13962547481060028\n",
      "In epoch 315, loss: 0.1354171335697174\n",
      "In epoch 320, loss: 0.13756293058395386\n",
      "In epoch 325, loss: 0.13296331465244293\n",
      "In epoch 330, loss: 0.1318993717432022\n",
      "In epoch 335, loss: 0.13056865334510803\n",
      "In epoch 340, loss: 0.12989865243434906\n",
      "In epoch 345, loss: 0.1314609944820404\n",
      "In epoch 350, loss: 0.1268957108259201\n",
      "In epoch 355, loss: 0.12437902390956879\n",
      "In epoch 360, loss: 0.1253717839717865\n",
      "In epoch 365, loss: 0.12179678678512573\n",
      "In epoch 370, loss: 0.12501077353954315\n",
      "In epoch 375, loss: 0.12028475850820541\n",
      "In epoch 380, loss: 0.11835748702287674\n",
      "In epoch 385, loss: 0.11843207478523254\n",
      "In epoch 390, loss: 0.11655635386705399\n",
      "In epoch 395, loss: 0.11604376137256622\n",
      "In epoch 400, loss: 0.1170215755701065\n",
      "In epoch 405, loss: 0.1120869517326355\n",
      "In epoch 410, loss: 0.1126229539513588\n",
      "In epoch 415, loss: 0.11087120324373245\n",
      "In epoch 420, loss: 0.11477026343345642\n",
      "In epoch 425, loss: 0.11307255178689957\n",
      "In epoch 430, loss: 0.10790055990219116\n",
      "In epoch 435, loss: 0.104035884141922\n",
      "In epoch 440, loss: 0.10572513937950134\n",
      "In epoch 445, loss: 0.10201288014650345\n",
      "In epoch 450, loss: 0.10366713255643845\n",
      "In epoch 455, loss: 0.10132057964801788\n",
      "In epoch 460, loss: 0.09834461659193039\n",
      "In epoch 465, loss: 0.1011306419968605\n",
      "In epoch 470, loss: 0.0962417796254158\n",
      "In epoch 475, loss: 0.0977354496717453\n",
      "In epoch 480, loss: 0.09844792634248734\n",
      "In epoch 485, loss: 0.09299580752849579\n",
      "In epoch 490, loss: 0.09659574180841446\n",
      "In epoch 495, loss: 0.09525638073682785\n",
      "AUC 0.9058664204370576\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    itertools.chain(model.parameters(), pred.parameters()), lr=0.01\n",
    ")\n",
    "\n",
    "all_logits = []\n",
    "for e in range(500):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata[\"feat\"])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print(\"AUC\", compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Evaluation (`with torch.no_grad():`):**\n",
    "   - Compute scores for positive and negative test edges using the trained model.\n",
    "   - Calculate probabilities from scores using sigmoid activation.\n",
    "   - Concatenate positive and negative probabilities along with true labels.\n",
    "   - Calculate the AUC-ROC score using the `roc_auc_score` function from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC-ROC Score: 0.9056571252693135\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Compute scores for positive and negative test edges\n",
    "    test_h = model(train_g, train_g.ndata[\"feat\"])\n",
    "    test_pos_score = pred(test_pos_g, test_h)\n",
    "    test_neg_score = pred(test_neg_g, test_h)\n",
    "\n",
    "    # Convert scores to probabilities using sigmoid\n",
    "    test_pos_prob = torch.sigmoid(test_pos_score)\n",
    "    test_neg_prob = torch.sigmoid(test_neg_score)\n",
    "\n",
    "    # Combine positive and negative probabilities\n",
    "    all_probs = torch.cat([test_pos_prob, test_neg_prob])\n",
    "    true_labels = torch.cat([torch.ones_like(test_pos_prob), torch.zeros_like(test_neg_prob)])\n",
    "\n",
    "    # Compute AUC-ROC score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc_roc_score = roc_auc_score(true_labels.cpu(), all_probs.cpu())\n",
    "\n",
    "    print(\"Test AUC-ROC Score:\", auc_roc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our AUC-ROC is 90.5 which is really good. This score generally means how well the model can distinguish betwee positive and negative\n",
    "edges in the graph. This means that our model is able to make the correct choice 90% of the time. This kind of score is generally what require\n",
    "for real life scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally saving th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ../model/dgl_model.pt\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"../model/dgl_model.pt\"  # Provide the desired path to save the model\n",
    "torch.save(model, model_save_path)\n",
    "print(\"Model saved at:\", model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
